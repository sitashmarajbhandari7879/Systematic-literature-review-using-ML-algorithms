{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e97f169",
   "metadata": {},
   "source": [
    "To cluster the documents and annotate the clusters according to the main topic, we can use various natural language \n",
    "processing and machine learning techniques. Here is a general framework that can be used to accomplish this task:\n",
    "\n",
    "1.Load the dataset into a pandas dataframe.\n",
    "2.Clean the data by removing any null values or duplicates.\n",
    "3.Preprocess the textual data (i.e., the abstracts) by removing stopwords, punctuation, and other noise. This can be done using various libraries like NLTK or spaCy.\n",
    "4.Convert the preprocessed abstracts into a numerical representation using a technique like TF-IDF or Doc2Vec.\n",
    "5. Find the relevant document \n",
    "6.To find the main topic of each document, we will use a topic modeling algorithm such as Latent Dirichlet Allocation (LDA) or Non-negative Matrix Factorization (NMF)\n",
    "6.For clustering and visualization, we will use a dimensionality reduction technique such as Principal Component Analysis (PCA) or t-SNE, and a visualization library such as bokeh.''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffc7e654",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sitas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sitas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "'''Packages for preprocessing'''\n",
    "import nltk \n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "\n",
    "'''Pakages to load dataset'''\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from bokeh.plotting import figure, show, output_file, ColumnDataSource\n",
    "from bokeh.models import HoverTool\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55824404",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Loading the dataset and subsetting it'''\n",
    "data = pd.read_csv(\"clean_dataset1.csv\",encoding='latin-1')\n",
    "# # Prompt the user to enter one or more search queries separated by commas\n",
    "# query_str = input('Enter one or more search queries separated by commas: ')\n",
    "# queries = [q.strip() for q in query_str.split(',')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64027b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document Title</th>\n",
       "      <th>Year</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>PDF Link</th>\n",
       "      <th>label</th>\n",
       "      <th>topic</th>\n",
       "      <th>topic_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>diagnostic maintenance technique using computer</td>\n",
       "      <td>1963</td>\n",
       "      <td>possible technique attending software need adv...</td>\n",
       "      <td>http://ieeexplore.ieee.org/stamp/stamp.jsp?arn...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>test, testing, case, coverage, suite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>high speed electro optical mechanical phototyp...</td>\n",
       "      <td>1968</td>\n",
       "      <td>adanced performance automated phototypesetting...</td>\n",
       "      <td>http://ieeexplore.ieee.org/stamp/stamp.jsp?arn...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>network, algorithm, performance, power, data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recognition handprinted numeral two stage feat...</td>\n",
       "      <td>1970</td>\n",
       "      <td>optical character recognition system handprint...</td>\n",
       "      <td>http://ieeexplore.ieee.org/stamp/stamp.jsp?arn...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>network, algorithm, performance, power, data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>computer diagnosis using blocking gate approach</td>\n",
       "      <td>1971</td>\n",
       "      <td>previous paper author considered application g...</td>\n",
       "      <td>http://ieeexplore.ieee.org/stamp/stamp.jsp?arn...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>fault, reliability, failure, tolerant, detection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>simulation modeling air quality control</td>\n",
       "      <td>1971</td>\n",
       "      <td>simulation modeling major role air quality pro...</td>\n",
       "      <td>http://ieeexplore.ieee.org/stamp/stamp.jsp?arn...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>software, metric, model, quality, defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5995</th>\n",
       "      <td>research relationship curvature radius deflect...</td>\n",
       "      <td>2011</td>\n",
       "      <td>china current specification design asphalt pav...</td>\n",
       "      <td>http://ieeexplore.ieee.org/stamp/stamp.jsp?arn...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>network, algorithm, performance, power, data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5996</th>\n",
       "      <td>sampling dmr practical low overhead permanent ...</td>\n",
       "      <td>2011</td>\n",
       "      <td>technology scaling manufacture time field perm...</td>\n",
       "      <td>http://ieeexplore.ieee.org/stamp/stamp.jsp?arn...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>fault, reliability, failure, tolerant, detection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5997</th>\n",
       "      <td>qos aware multipath routing protocol delay sen...</td>\n",
       "      <td>2011</td>\n",
       "      <td>paper proposes qos multipath routing protocol ...</td>\n",
       "      <td>http://ieeexplore.ieee.org/stamp/stamp.jsp?arn...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>network, algorithm, performance, power, data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5998</th>\n",
       "      <td>monitoring high performance data stream vertic...</td>\n",
       "      <td>2011</td>\n",
       "      <td>last several year monitoring high performance ...</td>\n",
       "      <td>http://ieeexplore.ieee.org/stamp/stamp.jsp?arn...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>network, algorithm, performance, power, data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999</th>\n",
       "      <td>rate distortion optimized image coding allowin...</td>\n",
       "      <td>2011</td>\n",
       "      <td>paper proposes efficient lossy image coding sc...</td>\n",
       "      <td>http://ieeexplore.ieee.org/stamp/stamp.jsp?arn...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>network, algorithm, performance, power, data</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6000 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Document Title  Year  \\\n",
       "0       diagnostic maintenance technique using computer  1963   \n",
       "1     high speed electro optical mechanical phototyp...  1968   \n",
       "2     recognition handprinted numeral two stage feat...  1970   \n",
       "3       computer diagnosis using blocking gate approach  1971   \n",
       "4               simulation modeling air quality control  1971   \n",
       "...                                                 ...   ...   \n",
       "5995  research relationship curvature radius deflect...  2011   \n",
       "5996  sampling dmr practical low overhead permanent ...  2011   \n",
       "5997  qos aware multipath routing protocol delay sen...  2011   \n",
       "5998  monitoring high performance data stream vertic...  2011   \n",
       "5999  rate distortion optimized image coding allowin...  2011   \n",
       "\n",
       "                                               Abstract  \\\n",
       "0     possible technique attending software need adv...   \n",
       "1     adanced performance automated phototypesetting...   \n",
       "2     optical character recognition system handprint...   \n",
       "3     previous paper author considered application g...   \n",
       "4     simulation modeling major role air quality pro...   \n",
       "...                                                 ...   \n",
       "5995  china current specification design asphalt pav...   \n",
       "5996  technology scaling manufacture time field perm...   \n",
       "5997  paper proposes qos multipath routing protocol ...   \n",
       "5998  last several year monitoring high performance ...   \n",
       "5999  paper proposes efficient lossy image coding sc...   \n",
       "\n",
       "                                               PDF Link  label  topic  \\\n",
       "0     http://ieeexplore.ieee.org/stamp/stamp.jsp?arn...      0      3   \n",
       "1     http://ieeexplore.ieee.org/stamp/stamp.jsp?arn...      0      1   \n",
       "2     http://ieeexplore.ieee.org/stamp/stamp.jsp?arn...      0      1   \n",
       "3     http://ieeexplore.ieee.org/stamp/stamp.jsp?arn...      0      2   \n",
       "4     http://ieeexplore.ieee.org/stamp/stamp.jsp?arn...      0      0   \n",
       "...                                                 ...    ...    ...   \n",
       "5995  http://ieeexplore.ieee.org/stamp/stamp.jsp?arn...      0      1   \n",
       "5996  http://ieeexplore.ieee.org/stamp/stamp.jsp?arn...      0      2   \n",
       "5997  http://ieeexplore.ieee.org/stamp/stamp.jsp?arn...      0      1   \n",
       "5998  http://ieeexplore.ieee.org/stamp/stamp.jsp?arn...      0      1   \n",
       "5999  http://ieeexplore.ieee.org/stamp/stamp.jsp?arn...      0      1   \n",
       "\n",
       "                                           topic_words  \n",
       "0                 test, testing, case, coverage, suite  \n",
       "1         network, algorithm, performance, power, data  \n",
       "2         network, algorithm, performance, power, data  \n",
       "3     fault, reliability, failure, tolerant, detection  \n",
       "4             software, metric, model, quality, defect  \n",
       "...                                                ...  \n",
       "5995      network, algorithm, performance, power, data  \n",
       "5996  fault, reliability, failure, tolerant, detection  \n",
       "5997      network, algorithm, performance, power, data  \n",
       "5998      network, algorithm, performance, power, data  \n",
       "5999      network, algorithm, performance, power, data  \n",
       "\n",
       "[6000 rows x 7 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"  In this block the tokens is vectorized then perform topic modeling\"\"\"\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "word_vector = vectorizer.fit_transform(data['Document Title'] + ' '+ data['Abstract'])\n",
    "\n",
    "# Normalize the feature matrix\n",
    "word_vector = normalize(word_vector)\n",
    "\n",
    "\n",
    "# Fit NMF model to the data\n",
    "nmf_model = NMF(n_components=5, init='nndsvd')\n",
    "nmf_model.fit(word_vector)\n",
    "\n",
    "# Add topic column to dataframe\n",
    "data['topic'] = nmf_model.transform(word_vector).argmax(axis=1)\n",
    "\n",
    "# get the top word for each topic\n",
    "topic_words = []\n",
    "n_words = 5\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(nmf_model.components_):\n",
    "    top_words_idx = topic.argsort()[:-n_words - 1:-1]\n",
    "    top_words = [feature_names[i] for i in top_words_idx]\n",
    "    topic_words.append(', '.join(top_words))\n",
    "\n",
    "data['topic_words'] = [topic_words[i] for i in data['topic']]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "accd050a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['test, testing, case, coverage, suite',\n",
       "       'network, algorithm, performance, power, data',\n",
       "       'fault, reliability, failure, tolerant, detection',\n",
       "       'software, metric, model, quality, defect',\n",
       "       'service, web, qos, application, resource'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.topic_words.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51e1d29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: software, metric, model, quality, defect\n",
      "Topic 1: network, algorithm, performance, power, data\n",
      "Topic 2: fault, reliability, failure, tolerant, detection\n",
      "Topic 3: test, testing, case, coverage, suite\n",
      "Topic 4: service, web, qos, application, resource\n"
     ]
    }
   ],
   "source": [
    "# Group the dataframe by the 'topic' column and aggregate the 'topic_word' column\n",
    "grouped = data.groupby('topic')['topic_words'].unique()\n",
    "\n",
    "# Print the unique values in the 'topic' column and their corresponding values in the 'topic_word' column\n",
    "for topic, words in grouped.items():\n",
    "    print(f\"Topic {topic}: {', '.join(words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13c8c20f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Perform dimensionality reduction using PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(word_vector.toarray())\n",
    "\n",
    "# Perform dimensionality reduction using t-SNE\n",
    "tsne = TSNE(n_components=2, perplexity=30, learning_rate=200, n_iter=1000, random_state=42)\n",
    "X_tsne = tsne.fit_transform(word_vector.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90d0eac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Bokeh plot\n",
    "source = ColumnDataSource(data=dict(\n",
    "    x=X_tsne[:,0],\n",
    "    y=X_tsne[:,1],\n",
    "    color=data['topic'].map({0:'red', 1:'blue', 2:'green', 3:'purple', 4:'orange'}),\n",
    "    topic=data['topic'],\n",
    "    topics=data['topic_words'],\n",
    "    title=data['Document Title'],\n",
    "    url=data['PDF Link'],\n",
    "    published=data['Year']\n",
    "))\n",
    "p = figure(title='Topic Clustering of Documents', plot_width=800, plot_height=800, tools='hover,box_zoom,reset')\n",
    "p.scatter(x='x', y='y', color='color', source=source, size=10, legend_group='topics')\n",
    "p.legend.title = 'Topic'\n",
    "p.hover.tooltips = [\n",
    "    ('Title', '@title'),\n",
    "    ('URL', '@url')\n",
    "]\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2305a427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-----------------------------------------------------------------------------------------------------------------'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''-----------------------------------------------------------------------------------------------------------------'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e26f4bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ndcg_score\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# load the dataset\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdataset.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# preprocess the data\u001b[39;00m\n\u001b[0;32m     12\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mdropna()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:934\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    931\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 934\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1218\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1217\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1218\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1228\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1229\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py:786\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    785\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 786\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    793\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    794\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    795\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset.csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7608d2c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f9ff7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04110812",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7360ab99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2eed72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538f3c45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a07046",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
